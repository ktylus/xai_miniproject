{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import sklearn.metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from datasets import CaliforniaHousingDataset, AdultDataset, TitanicDataset, AutoMpgDataset, WineDataset\n",
    "from metrics import calculate_global_fidelity, calculate_global_neighborhood_fidelity\n",
    "from models.base_model import BaseClassifier, BaseRegressor\n",
    "from models.surrogate_model import SurrogateClassifier, SurrogateRegressor\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "housing_train = CaliforniaHousingDataset(\n",
    "    dataset_path=\"data/california_housing/cal_housing.data\", normalize=True, train=True)\n",
    "housing_test = CaliforniaHousingDataset(\n",
    "    dataset_path=\"data/california_housing/cal_housing.data\", normalize=True, train=False)\n",
    "\n",
    "adult_train = AdultDataset(dataset_path=\"data/adult/adult.data\", normalize=True, train=True)\n",
    "adult_test = AdultDataset(dataset_path=\"data/adult/adult.data\", normalize=True, train=False)\n",
    "\n",
    "titanic_train = TitanicDataset(dataset_path=\"data/titanic/titanic.arff\", normalize=True, train=True)\n",
    "titanic_test = TitanicDataset(dataset_path=\"data/titanic/titanic.arff\", normalize=True, train=False)\n",
    "\n",
    "wine_train = WineDataset(dataset_path=\"data/wines/winequality-red.csv\", normalize=True, train=True)\n",
    "wine_test = WineDataset(dataset_path=\"data/wines/winequality-red.csv\", normalize=True, train=False)\n",
    "\n",
    "autompg_train = AutoMpgDataset(dataset_path=\"data/autompg/auto-mpg.data\", normalize=True, train=True)\n",
    "autompg_test = AutoMpgDataset(dataset_path=\"data/autompg/auto-mpg.data\", normalize=True, train=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# target normalization just to check (the mse scores in the paper are low - what's the transformation / metric used?)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# after seeing the results, it seems the authors just put the target through a StandardScaler\n",
    "housing_train.target = pd.Series(scaler.fit_transform(np.array(housing_train.target).reshape(-1, 1)).flatten())\n",
    "housing_test.target = pd.Series(scaler.fit_transform(np.array(housing_test.target).reshape(-1, 1)).flatten())\n",
    "wine_train.target = pd.Series(scaler.fit_transform(np.array(wine_train.target).reshape(-1, 1)).flatten())\n",
    "wine_test.target = pd.Series(scaler.fit_transform(np.array(wine_test.target).reshape(-1, 1)).flatten())\n",
    "autompg_train.target = pd.Series(scaler.fit_transform(np.array(autompg_train.target).reshape(-1, 1)).flatten())\n",
    "autompg_test.target = pd.Series(scaler.fit_transform(np.array(autompg_test.target).reshape(-1, 1)).flatten())\n",
    "\n",
    "# TODO move that to datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 128  # not from the paper\n",
    "binary_classification_criterion = torch.nn.BCELoss()\n",
    "regression_criterion = ... # \"logarithm of the hyperbolic cosine\" from the paper (?)\n",
    "regression_criterion = torch.nn.MSELoss()\n",
    "# TODO early stopping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "def train(\n",
    "        base_model: nn.Module,\n",
    "        surrogate_model: nn.Module,\n",
    "        train_data: Dataset,\n",
    "        criterion,\n",
    "        epochs: int,\n",
    "        alpha: float\n",
    "):\n",
    "    params = list(base_model.parameters()) + list(surrogate_model.parameters())\n",
    "    optimizer = Adam(params, lr=lr)\n",
    "    loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for data, labels in loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            labels = labels.reshape(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            base_model_preds = base_model(data)\n",
    "            surrogate_model_preds = surrogate_model(data)\n",
    "            loss = criterion(base_model_preds, labels)\n",
    "            point_fidelity = calculate_global_fidelity(base_model_preds, surrogate_model_preds)\n",
    "            mtl_loss = alpha * loss + (1 - alpha) * point_fidelity\n",
    "\n",
    "            mtl_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += mtl_loss\n",
    "        print(f\"epoch: {epoch + 1}, train loss: {running_loss / len(loader):.3f}\")\n",
    "\n",
    "\n",
    "def validate_base_classifier(\n",
    "        model: nn.Module,\n",
    "        test_data: Dataset,\n",
    "):\n",
    "    loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "    with torch.no_grad():\n",
    "        data, labels = next(iter(loader))\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        labels = labels.reshape(-1, 1)\n",
    "        preds_proba = model(data)\n",
    "        preds = torch.where(preds_proba >= 0.5, 1, 0)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1_score = sklearn.metrics.f1_score(labels.cpu(), preds.cpu())\n",
    "        print(f\"test accuracy: {accuracy:.3f}, f1 score: {f1_score:.3f}\")\n",
    "\n",
    "\n",
    "def validate_base_regressor(\n",
    "        model: nn.Module,\n",
    "        test_data: Dataset\n",
    "):\n",
    "    loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "    with torch.no_grad():\n",
    "        data, labels = next(iter(loader))\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        labels = labels.reshape(-1, 1)\n",
    "        preds = model(data)\n",
    "        mse = sklearn.metrics.mean_squared_error(labels.cpu(), preds.cpu())\n",
    "        print(f\"test mse: {mse:.3f}\")\n",
    "\n",
    "\n",
    "def validate_surrogate_model(\n",
    "        base_model: nn.Module,\n",
    "        surrogate_model: nn.Module,\n",
    "        test_data: Dataset\n",
    "):\n",
    "    loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "    with torch.no_grad():\n",
    "        data, _ = next(iter(loader))\n",
    "        data = data.to(device)\n",
    "        base_model_preds = base_model(data)\n",
    "        surrogate_model_preds = surrogate_model(data)\n",
    "        global_fidelity = calculate_global_fidelity(base_model_preds, surrogate_model_preds)\n",
    "        global_neighborhood_fidelity = calculate_global_neighborhood_fidelity(base_model, surrogate_model, data)\n",
    "        print(f\"global fidelity: {global_fidelity:.3f}, global neighborhood fidelity: {global_neighborhood_fidelity:.3f}\")\n",
    "\n",
    "\n",
    "def validate_regressors(\n",
    "        base_model: nn.Module,\n",
    "        surrogate_model: nn.Module,\n",
    "        test_data: Dataset\n",
    "):\n",
    "    validate_base_regressor(base_model, test_data)\n",
    "    validate_surrogate_model(base_model, surrogate_model, test_data)\n",
    "\n",
    "\n",
    "def validate_classifiers(\n",
    "        base_model: nn.Module,\n",
    "        surrogate_model: nn.Module,\n",
    "        test_data: Dataset\n",
    "):\n",
    "    validate_base_classifier(base_model, test_data)\n",
    "    validate_surrogate_model(base_model, surrogate_model, test_data)\n",
    "\n",
    "# TODO local explainability evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult:\n",
      "epoch: 1, train loss: 0.219\n",
      "epoch: 2, train loss: 0.180\n",
      "epoch: 3, train loss: 0.170\n",
      "epoch: 4, train loss: 0.164\n",
      "epoch: 5, train loss: 0.160\n",
      "\n",
      "titanic:\n",
      "epoch: 1, train loss: 0.347\n",
      "epoch: 2, train loss: 0.314\n",
      "epoch: 3, train loss: 0.282\n",
      "epoch: 4, train loss: 0.273\n",
      "epoch: 5, train loss: 0.265\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "alpha = 0.5\n",
    "\n",
    "classification_data = {\n",
    "    \"adult\": (adult_train, adult_test),\n",
    "    \"titanic\": (titanic_train, titanic_test)\n",
    "}\n",
    "classifiers = []\n",
    "\n",
    "for dataset in classification_data.keys():\n",
    "    train_data, test_data = classification_data[dataset]\n",
    "    base_model = BaseClassifier(\n",
    "        input_dim=train_data.features.shape[1], output_dim=1, n_hidden_layers=4, layer_size=128).to(device)\n",
    "    surrogate_model = SurrogateClassifier(input_dim=train_data.features.shape[1], output_dim=1).to(device)\n",
    "    print(f\"{dataset}:\")\n",
    "    train(base_model, surrogate_model, train_data, binary_classification_criterion, epochs, alpha)\n",
    "    classifiers.append((base_model, surrogate_model))\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult:\n",
      "test accuracy: 0.863, f1 score: 0.694\n",
      "global fidelity: 0.016, global neighborhood fidelity: 0.016\n",
      "\n",
      "titanic:\n",
      "test accuracy: 0.780, f1 score: 0.750\n",
      "global fidelity: 0.055, global neighborhood fidelity: 0.055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, dataset in enumerate(classification_data.keys()):\n",
    "    base_model, surrogate_model = classifiers[i]\n",
    "    test_data = classification_data[dataset][1]\n",
    "    print(f\"{dataset}:\")\n",
    "    validate_classifiers(base_model, surrogate_model, test_data)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine:\n",
      "epoch: 1, train loss: 0.605\n",
      "epoch: 2, train loss: 0.519\n",
      "epoch: 3, train loss: 0.487\n",
      "epoch: 4, train loss: 0.467\n",
      "epoch: 5, train loss: 0.448\n",
      "\n",
      "housing:\n",
      "epoch: 1, train loss: 0.417\n",
      "epoch: 2, train loss: 0.323\n",
      "epoch: 3, train loss: 0.274\n",
      "epoch: 4, train loss: 0.238\n",
      "epoch: 5, train loss: 0.211\n",
      "\n",
      "autompg:\n",
      "epoch: 1, train loss: 0.595\n",
      "epoch: 2, train loss: 0.540\n",
      "epoch: 3, train loss: 0.485\n",
      "epoch: 4, train loss: 0.450\n",
      "epoch: 5, train loss: 0.438\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regression_data = {\n",
    "    \"wine\": (wine_train, wine_test),\n",
    "    \"housing\": (housing_train, housing_test),\n",
    "    \"autompg\": (autompg_train, autompg_test)\n",
    "}\n",
    "regressors = []\n",
    "\n",
    "for dataset in regression_data.keys():\n",
    "    train_data, test_data = regression_data[dataset]\n",
    "    base_regressor = BaseRegressor(\n",
    "        input_dim=train_data.features.shape[1], output_dim=1, n_hidden_layers=4, layer_size=128).to(device)\n",
    "    surrogate_regressor = SurrogateRegressor(input_dim=train_data.features.shape[1], output_dim=1).to(device)\n",
    "    print(f\"{dataset}:\")\n",
    "    train(base_regressor, surrogate_regressor, train_data, regression_criterion, epochs, alpha)\n",
    "    regressors.append((base_regressor, surrogate_regressor))\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wine:\n",
      "test mse: 0.742\n",
      "global fidelity: 0.153, global neighborhood fidelity: 0.154\n",
      "\n",
      "housing:\n",
      "test mse: 0.315\n",
      "global fidelity: 0.086, global neighborhood fidelity: 0.093\n",
      "\n",
      "autompg:\n",
      "test mse: 0.510\n",
      "global fidelity: 0.420, global neighborhood fidelity: 0.423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, dataset in enumerate(regression_data.keys()):\n",
    "    base_model, surrogate_model = regressors[i]\n",
    "    test_data = regression_data[dataset][1]\n",
    "    print(f\"{dataset}:\")\n",
    "    validate_regressors(base_model, surrogate_model, test_data)\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
