{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import sklearn.metrics\n",
    "\n",
    "from datasets import CaliforniaHousingDataset, AdultDataset, TitanicDataset, AutoMpgDataset, WineDataset\n",
    "from metrics import calculate_global_fidelity, calculate_global_neighborhood_fidelity\n",
    "from models.base_model import BaseClassifier, BaseRegressor\n",
    "from models.surrogate_model import SurrogateClassifier, SurrogateRegressor\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "housing_train = CaliforniaHousingDataset(\n",
    "    dataset_path=\"data/california_housing/cal_housing.data\", normalize=True, train=True)\n",
    "housing_test = CaliforniaHousingDataset(\n",
    "    dataset_path=\"data/california_housing/cal_housing.data\", normalize=True, train=False)\n",
    "\n",
    "#adult_train = AdultDataset(dataset_path=\"data/adult/adult.data\", normalize=True, train=True)\n",
    "#adult_test = AdultDataset(dataset_path=\"data/adult/adult.data\", normalize=True, train=False)\n",
    "\n",
    "titanic_train = TitanicDataset(dataset_path=\"data/titanic/titanic.arff\", normalize=True, train=True)\n",
    "titanic_test = TitanicDataset(dataset_path=\"data/titanic/titanic.arff\", normalize=True, train=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "housing_train.features = housing_train.features.astype(\"float32\")\n",
    "housing_train.target = housing_train.target.astype(\"float32\")\n",
    "\n",
    "housing_test.features = housing_test.features.astype(\"float32\")\n",
    "housing_test.target = housing_test.target.astype(\"float32\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "titanic_train.features = titanic_train.features.astype('float32')\n",
    "titanic_train.target = titanic_train.target.astype(\"float32\")\n",
    "\n",
    "titanic_test.features = titanic_test.features.astype(\"float32\")\n",
    "titanic_test.target = titanic_test.target.astype(\"float32\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 128  # not from the paper\n",
    "binary_classification_criterion = torch.nn.BCELoss()\n",
    "regression_criterion = ... # \"logarithm of the hyperbolic cosine\" from the paper (?)\n",
    "regression_criterion = torch.nn.MSELoss()\n",
    "# TODO early stopping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def train(\n",
    "        base_model: nn.Module,\n",
    "        surrogate_model: nn.Module,\n",
    "        train_data: Dataset,\n",
    "        test_data: Dataset,\n",
    "        criterion,\n",
    "        epochs: int,\n",
    "        alpha: float\n",
    "):\n",
    "    params = list(base_model.parameters()) + list(surrogate_model.parameters())\n",
    "    optimizer = Adam(params, lr=lr)\n",
    "    loader = DataLoader(train_data, batch_size=batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        for data, labels in loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            labels = labels.reshape(-1, 1)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            base_model_preds = base_model(data)\n",
    "            surrogate_model_preds = surrogate_model(data)\n",
    "            loss = criterion(base_model_preds, labels)\n",
    "            point_fidelity = calculate_global_fidelity(base_model_preds, surrogate_model_preds)\n",
    "            mtl_loss = alpha * loss + (1 - alpha) * point_fidelity\n",
    "\n",
    "            mtl_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += mtl_loss\n",
    "        print(f\"epoch: {epoch}, train loss: {running_loss / len(loader):.4f}\")\n",
    "\n",
    "\n",
    "def validate_base_classifier(\n",
    "        model: nn.Module,\n",
    "        test_data: Dataset,\n",
    "):\n",
    "    loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "    with torch.no_grad():\n",
    "        data, labels = next(iter(loader))\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        labels = labels.reshape(-1, 1)\n",
    "        preds_proba = model(data)\n",
    "        preds = torch.where(preds_proba >= 0.5, 1, 0)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels.cpu(), preds.cpu())\n",
    "        f1_score = sklearn.metrics.f1_score(labels.cpu(), preds.cpu())\n",
    "        print(f\"test accuracy: {accuracy:.4f}, f1 score: {f1_score:.4f}\")\n",
    "\n",
    "\n",
    "def validate_base_regressor(\n",
    "        model: nn.Module,\n",
    "        test_data: Dataset\n",
    "):\n",
    "    loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "    with torch.no_grad():\n",
    "        data, labels = next(iter(loader))\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        labels = labels.reshape(-1, 1)\n",
    "        preds = model(data)\n",
    "        mse = sklearn.metrics.mean_squared_error(labels.cpu(), preds.cpu())\n",
    "        print(f\"test mse: {mse:.4f}\")\n",
    "\n",
    "\n",
    "def validate_surrogate_model(\n",
    "        base_model: nn.Module,\n",
    "        surrogate_model: nn.Module,\n",
    "        test_data: Dataset\n",
    "):\n",
    "    loader = DataLoader(test_data, batch_size=len(test_data))\n",
    "    with torch.no_grad():\n",
    "        data, _ = next(iter(loader))\n",
    "        data = data.to(device)\n",
    "        base_model_preds = base_model(data)\n",
    "        surrogate_model_preds = surrogate_model(data)\n",
    "        global_fidelity = calculate_global_fidelity(base_model_preds, surrogate_model_preds)\n",
    "        global_neighborhood_fidelity = calculate_global_neighborhood_fidelity(base_model, surrogate_model, data)\n",
    "        print(f\"global fidelity: {global_fidelity}, global neighborhood fidelity: {global_neighborhood_fidelity}\")\n",
    "\n",
    "\n",
    "def validate_regressors(\n",
    "        base_model: nn.Module,\n",
    "        surrogate_model: nn.Module,\n",
    "        test_data: Dataset\n",
    "):\n",
    "    validate_base_regressor(base_model, test_data)\n",
    "    validate_surrogate_model(base_model, surrogate_model, test_data)\n",
    "\n",
    "\n",
    "def validate_classifiers(\n",
    "        base_model: nn.Module,\n",
    "        surrogate_model: nn.Module,\n",
    "        test_data: Dataset\n",
    "):\n",
    "    validate_base_classifier(base_model, test_data)\n",
    "    validate_surrogate_model(base_model, surrogate_model, test_data)\n",
    "\n",
    "# TODO local explainability evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.6159\n",
      "epoch: 1, train loss: 0.5436\n",
      "epoch: 2, train loss: 0.4563\n",
      "epoch: 3, train loss: 0.4295\n",
      "epoch: 4, train loss: 0.4111\n",
      "epoch: 5, train loss: 0.4054\n",
      "epoch: 6, train loss: 0.3998\n",
      "epoch: 7, train loss: 0.3919\n",
      "epoch: 8, train loss: 0.3884\n",
      "epoch: 9, train loss: 0.3852\n"
     ]
    }
   ],
   "source": [
    "base_model = BaseClassifier(input_dim=titanic_train.features.shape[1], output_dim=1, n_hidden_layers=4, layer_size=128).to(device)\n",
    "surrogate_model = SurrogateClassifier(input_dim=titanic_train.features.shape[1], output_dim=1).to(device)\n",
    "\n",
    "train(base_model, surrogate_model, titanic_train, titanic_test, binary_classification_criterion, 10, 0.9)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.8278, f1 score: 0.7931\n",
      "global fidelity: 0.14865344762802124, global neighborhood fidelity: 0.14915680885314941\n"
     ]
    }
   ],
   "source": [
    "validate_classifiers(base_model, surrogate_model, titanic_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 25443661824.0000\n",
      "epoch: 1, train loss: 16561262592.0000\n",
      "epoch: 2, train loss: 15765088256.0000\n",
      "epoch: 3, train loss: 15460441088.0000\n",
      "epoch: 4, train loss: 15289368576.0000\n",
      "epoch: 5, train loss: 15198922752.0000\n",
      "epoch: 6, train loss: 15152428032.0000\n",
      "epoch: 7, train loss: 15126321152.0000\n",
      "epoch: 8, train loss: 15107948544.0000\n",
      "epoch: 9, train loss: 15092577280.0000\n"
     ]
    }
   ],
   "source": [
    "base_regressor = BaseRegressor(input_dim=housing_train.features.shape[1], output_dim=1, n_hidden_layers=4, layer_size=128).to(device)\n",
    "surrogate_regressor = SurrogateRegressor(input_dim=housing_train.features.shape[1], output_dim=1).to(device)\n",
    "\n",
    "train(base_regressor, surrogate_regressor, housing_train, housing_test, regression_criterion, 10, 0.5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mse: 16953387008.0000\n",
      "global fidelity: 12593926144.0, global neighborhood fidelity: 12737275904.0\n"
     ]
    }
   ],
   "source": [
    "# TODO normalize something?\n",
    "\n",
    "validate_regressors(base_regressor, surrogate_regressor, housing_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
